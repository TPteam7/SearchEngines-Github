{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "Is there a difference between an ANN and an MLP? Explain the two concepts and what, if anything, they have in common. Include illustrative diagrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "ANN stands for an Artifical Neural Network that is made up of interconnected neurons arranged in layers.\n",
    "\n",
    "MLP stands for a Multilayer Perceptron is a specific type of ANN that has at least three layers of input, hidden, and output.\n",
    "\n",
    "The difference would be that an MLP is a specific type of ANN so there are different types of architectures such as recurrent that work differently.\n",
    "\n",
    "They both have everything in common that an MLP does though such as weights and bias, being neural networks, and using neurons and layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "What is meant by backpropagation in the context of a neural network? Support your explanation with diagrams and short code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "1. **Forward Pass**: Compute the output of the neural network by passing the input data through each of the layers\n",
    "2. **Compute Error**: Find the loss by doing actual - predicted\n",
    "3. **Backward Pass**: Propagate the error back through the network, calculating the gradient of the error with respect to each weight. This finds how much each weight contributes to the error\n",
    "4. **Update Weights**: Adjust the weights using the calculated gradients to minimize the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01303497]\n",
      " [0.98886061]\n",
      " [0.98886669]\n",
      " [0.01145979]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example of a simple neural network with one hidden layer\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Input dataset\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "# Output dataset\n",
    "outputs = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Initialize weights randomly with mean 0\n",
    "input_layer_neurons = inputs.shape[1]\n",
    "hidden_layer_neurons = 2\n",
    "output_neurons = 1\n",
    "\n",
    "hidden_weights = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
    "hidden_bias = np.random.uniform(size=(1, hidden_layer_neurons))\n",
    "output_weights = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n",
    "output_bias = np.random.uniform(size=(1, output_neurons))\n",
    "\n",
    "# Training algorithm\n",
    "for _ in range(10000):\n",
    "    # Forward pass to find the predicted output\n",
    "    hidden_layer_activation = np.dot(inputs, hidden_weights)\n",
    "    hidden_layer_activation += hidden_bias\n",
    "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "    output_layer_activation = np.dot(hidden_layer_output, output_weights)\n",
    "    output_layer_activation += output_bias\n",
    "    predicted_output = sigmoid(output_layer_activation)\n",
    "\n",
    "    # Compute error. This is the actual - predicted of the second step\n",
    "    error = outputs - predicted_output\n",
    "\n",
    "    # Backward pass. This finds how much each weight contributed to the error\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "    error_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # Update weights and biases for step 4\n",
    "    output_weights += hidden_layer_output.T.dot(d_predicted_output)\n",
    "    output_bias += np.sum(d_predicted_output, axis=0, keepdims=True)\n",
    "    hidden_weights += inputs.T.dot(d_hidden_layer)\n",
    "    hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
    "\n",
    "print(predicted_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
