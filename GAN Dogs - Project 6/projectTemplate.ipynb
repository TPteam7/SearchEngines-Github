{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .title {\n",
    "        font-family: 'Georgia', serif;\n",
    "        text-align: center;\n",
    "        margin-top: 40px;\n",
    "    }\n",
    "\n",
    "    .title h1 {\n",
    "        font-weight: 700;\n",
    "        font-size: 48px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        line-height: 1.2;\n",
    "    }\n",
    "\n",
    "    .title p {\n",
    "        font-size: 18px;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "\n",
    "    .title hr {\n",
    "        border: none;\n",
    "        border-top: 1px solid #BBBBBB; /* Light grey for light mode */\n",
    "        width: 60%;\n",
    "        margin: 20px auto;\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .title h1 {\n",
    "            color: #F5F5F5; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .title p {\n",
    "            color: #BBBBBB; /* Lighter grey for dark mode */\n",
    "        }\n",
    "\n",
    "        .title hr {\n",
    "            border-top: 1px solid #555555; /* Dark grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"title\">\n",
    "    <h1>Generative Adversarial Network - Dogs</h1>\n",
    "    <p>By: Trevor Pope and Micheal Callahan</p>\n",
    "    <hr>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Problem Statement</h2>\n",
    "    <p>\n",
    "        Fake images have become so ubiquitous on social media that they affect the public discourse on these platforms. It is essential for students to understand how these images are created.\n",
    "    </p>\n",
    "    <p>\n",
    "        We addressed this problem by using a type of GAN called ProGAN which was released in 2017 by researchers from NVIDIA AI\n",
    "    </p>\n",
    "    <p>\n",
    "        This is was done in collaboration from a tutorial on Youtube at the link in the references\n",
    "    </p>\n",
    "    <p>\n",
    "        We attempted to generate images of dogs that would \"fool\" the discriminator to generate images that looked just as real as the real images that the discrimantor was trained on.\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Algorithm</h2>\n",
    "    <p>\n",
    "        <img src=\"ProGAN.gif\" alt=\"Example GIF\">\n",
    "    </p>\n",
    "    <p>\n",
    "        The algorithm reflects progressive growing by starting with a 4Ã—4 pixel image and then adding more layers to increase the resolution.\n",
    "    </p>\n",
    "    <p>\n",
    "        This approach helps the model first learn low-resolution features and gradually focus on finer details to improve image quality at higher resolutions. It also stabilizes training by simplifying the task in early stages, allowing for better feature learning.\n",
    "    </p>\n",
    "    <p>\n",
    "        To ensure smooth transitions, the model uses blending when new layers are introduced, gradually incorporating higher resolution features without disrupting learned patterns. The discriminator grows alongside the generator, maintaining the ability to evaluate images at each resolution.\n",
    "    </p>\n",
    "    <p>\n",
    "        This is a broad overview, and we will go over the exact architecture of the generator and discriminator when we build them later.\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Data Initialization</h2>\n",
    "    <p>\n",
    "        The dataset chosen is a Kaggle dataset titled \"Dog Face Recognition,\" linked in the references below.\n",
    "    </p>\n",
    "    <p>\n",
    "        This dataset was selected because it contains images of dogs captured with consistent camera framing, which is critical for effective training. Similar framing ensures that the generator and discriminator can focus on learning meaningful visual features rather than compensating for variations in alignment or perspective. This is why celebrity datasets are also popular, as celebrity photos typically have consistent framing and styling.\n",
    "    </p>\n",
    "    <p>\n",
    "        While this dataset was originally designed for a CNN to extract features and identify individual dogs, we repurposed it for ProGAN. Specifically, we use the dataset to train the discriminator to recognize real dog images. The consistent format also allows us to include multiple breeds, enabling the model to generate diverse yet coherent outputs in the same framing style.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Data Preprocessing</h2>\n",
    "    <p>\n",
    "        For preprocessing, the dataset already performed some data augmentation, such as rotating images and flipping them horizontally. As a result, the only additional step needed was resizing the images to a consistent size before storing them in a separate folder.\n",
    "    </p>\n",
    "    <p>\n",
    "        We then initialize all the key variables for the model, with the most important being the progressive image resolution. We start with a 4x4 image and gradually scale up to a 128x128 image in 6 steps, allowing the model to first learn the coarse features and then focus on finer details. The image resolution and other parameters can be adjusted based on available computational power.\n",
    "    </p>\n",
    "    <p>\n",
    "        Next, we implement helper functions for saving and loading model checkpoints. We also include the gradient penalty function, which ensures that the critic (discriminator) responds smoothly to small changes in the input, preventing it from becoming overly sensitive to noise.\n",
    "    </p>\n",
    "    <p>\n",
    "        Given the progressive nature of training, the total number of epochs is relatively smallâ€”around 30 in total. However, as the model grows in complexity through the addition of new layers, each epoch requires significantly more computational power.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first image in the dataset\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# all_images = []\n",
    "\n",
    "# for folder in os.listdir('train'):\n",
    "#     for image in os.listdir('train/' + folder):\n",
    "#         img = cv.imread('train/' + folder + '/' + image)\n",
    "#         img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "#         img = cv.resize(img, (512, 512))\n",
    "\n",
    "#         # add the image to all_images\n",
    "#         all_images.append(img)\n",
    "\n",
    "# len(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, img in enumerate(all_images):\n",
    "#     cv.imwrite('data/all_images/' + str(i) + '.jpg', cv.cvtColor(img, cv.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import log2\n",
    "\n",
    "START_TRAIN_AT_IMG_SIZE = 4    # This is the size (4 x 4) of the images that the training will start at\n",
    "image_dataset = 'data'\n",
    "CHECKPOINT_GEN = \"models/generator.pth\" # The path to the generator checkpoint that will be used for inference\n",
    "CHECKPOINT_DISC = \"models/discriminator.pth\" # The path to the discriminator checkpoint that will be used for inference\n",
    "Save_Model = True\n",
    "Load_Model = False\n",
    "Learning_Rate = 2e-3\n",
    "Batch_Sizes = [16, 8, 8, 8, 4, 4] # The length decreases as the image size increases since more memory is needed\n",
    "Image_Size = 128    # The size of the images that the model will train up to\n",
    "Channel = 3         # The number of channels in the images (RGB)\n",
    "Z_DIM = 128        # The dimension of the noise vector, used for the generator input\n",
    "IN_CHANNELS = 128   # The number of feautre channels captured for the input to the generator\n",
    "LAMDA_GP = 5       # Gradient penalty lambda hyperparameter. Stabalizes the training for the GAN\n",
    "NUM_STEPS = int(log2(Image_Size/4)) + 1   # Calulates the number of steps needed to get to the final image size.\n",
    "\n",
    "PROGRESSIVE_EPOCHS = [5] * len(Batch_Sizes) # 5 epochs for each image size\n",
    "FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1) # The random noise that will be used in generator\n",
    "NUM_WORKERS = 6 # run command wmic cpu get NumberOfCores,NumberOfLogicalProcessors (in cmd). \n",
    "                #    We can use 80% of the cores + processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    #\n",
    "    #   Purpose: Calculate the gradient penalty for the WGAN-GP loss function\n",
    "    #               We are ensuring that the discriminator is being heavily impacted by small noise changes              \n",
    "    #\n",
    "    #   Inputs:\n",
    "    #       critic: The discriminator model\n",
    "    #       real: The real images\n",
    "    #       fake: The fake images\n",
    "    #       alpha: The interpolation parameter. T\n",
    "    #       train_step: The current training step\n",
    "    #       device: The device to run the calculations on\n",
    "\n",
    "    # Create a random mix of real and fake images.\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "# def generate_examples(gen, steps, truncation=0.7, n=100):\n",
    "#     \"\"\"\n",
    "#     Tried using truncation trick here but not sure it actually helped anything, you can\n",
    "#     remove it if you like and just sample from torch.randn\n",
    "#     \"\"\"\n",
    "#     gen.eval()\n",
    "#     alpha = 1.0\n",
    "#     for i in range(n):\n",
    "#         with torch.no_grad():\n",
    "#             noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, config.Z_DIM, 1, 1)), device=config.DEVICE, dtype=torch.float32)\n",
    "#             img = gen(noise, alpha, steps)\n",
    "#             save_image(img*0.5+0.5, f\"saved_examples/img_{i}.png\")\n",
    "#     gen.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Build the Model</h2>\n",
    "    <p>\n",
    "        Three inital parts of the model before Generator and Discriminator: WSConv2d Layer, Pixel-Wise Normalization, and Convolutional Block\n",
    "    </p>\n",
    "    <ul>\n",
    "        <li>\n",
    "            Weight Scaling (WSConv2d): Ensures stable training by controlling the weight magnitudes, helping both the generator and discriminator avoid instability.\n",
    "        </li>\n",
    "        <li>\n",
    "            Pixel Normalization (PixelNorm): Prevents runaway activations, ensuring the generator produces stable images and the discriminator can effectively differentiate between real and generated images.\n",
    "        </li>\n",
    "        <li>\n",
    "            Convolutional Block (ConvBlock): Extracts and refines features for both generating images (in the generator) and classifying images (in the discriminator), while maintaining stable gradients through the use of Leaky ReLU and PixelNorm.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import log2\n",
    "\n",
    "# factors to scale the noise inputs\n",
    "factors = [1,1,1,1, 1/2, 1/4, 1/8, 1/16, 1/32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator: Helps the generator learn and synthesize progressively more detailed images by applying controlled transformations to the noise and upscaled feature maps.\n",
    "\n",
    "Discriminator: Assists the discriminator in learning discriminative features from real and generated images, while preventing instability due to overly large weights.\n",
    "\n",
    "**Key Steps**:\n",
    "\n",
    "Weight Scaling: Controls the magnitude of weights to prevent them from becoming too large.\n",
    "\n",
    "No Bias Initially: Bias is removed initially and added back after the scaling operation to maintain model stability.\n",
    "\n",
    "Normalization of Weights: The layer is initialized with normal distribution, which ensures that the learning process starts with a well-conditioned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    # Weight scaled Conv2d\n",
    "    # Purpose: to scale the weights of the convolutional layer by a constant factor to prevent the weights from becoming too large\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "\n",
    "        # in_channels: number of input channels\n",
    "        # out_channels: number of output channels\n",
    "        # kernel_size: size of the kernel\n",
    "        # stride: stride of the convolution\n",
    "        # padding: padding of the convolution\n",
    "        # gain: gain factor to scale the weights by\n",
    "\n",
    "        super(WSConv2d, self).__init__()\n",
    "        \n",
    "        # create a convolutional layer\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "        # calculate the scale factor for the weights\n",
    "        #   Ensures that weights have a controlled gain\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "        \n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    # This is to compute the transformed output of the layer given the input x\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator: In the generator, PixelNorm helps normalize the latent vector, ensuring that the generator is not overly sensitive to any particular part of the latent space, which would result in less realistic image generation.\n",
    "\n",
    "Discriminator: For the discriminator, normalizing the input images helps in distinguishing real from fake images without bias toward specific pixel values or features, leading to better feature extraction and more reliable predictions.\n",
    "\n",
    "**Key Steps**:\n",
    "\n",
    "Mean Squared Normalization: It computes the mean of the squared values across the channels for each pixel and normalizes accordingly.\n",
    "\n",
    "Small Epsilon: To prevent division by zero, a small epsilon value is added during normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    # Pixelwise feature vector normalization\n",
    "    # Purpose: to normalize the feature vectors of the input image\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "    \n",
    "    # This is to compute the transformed output of the layer given the input x\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator: The generator requires the ability to gradually refine the feature maps from random noise into a structured image. Each ConvBlock helps the generator gradually increase the level of detail and resolution by progressively applying convolutions.\n",
    "\n",
    "Discriminator: The discriminator uses ConvBlocks to extract features that distinguish real images from fake ones. By stacking these blocks, the discriminator learns progressively higher-level features of images, which is important for making accurate predictions in high-resolution images.\n",
    "\n",
    "**Key Steps**:\n",
    "\n",
    "Leaky ReLU Activation: Helps to avoid the vanishing gradient problem by allowing a small, non-zero gradient when the input is negative.\n",
    "\n",
    "Two Convolutions: Two successive convolutions allow for learning more complex hierarchical features.\n",
    "\n",
    "Pixel Normalization: Optional but recommended for stable training. It normalizes the features between each convolution to improve stability and convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    # Convolutional block\n",
    "    # Purpose: to apply two convolutional layers with leaky ReLU activation functions\n",
    "    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n",
    "        # in_channels: number of input channels\n",
    "        # out_channels: number of output channels\n",
    "        # use_pixelnorm: flag to determine if pixel normalization should be applied\n",
    "        \n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "        self.use_pixelnorm = use_pixelnorm\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pixelnorm else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pixelnorm else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Summary of Key Components:</h2>**\n",
    "\n",
    "Initial Block (initial): Transforms the latent vector into a small image, starting the generation process.\n",
    "\n",
    "Progressive Blocks (prog_blocks): Handle the progressive growth of the image resolution, adding more complexity at each step.\n",
    "\n",
    "Fade-in (fade_in): Smoothly blends lower-resolution and higher-resolution outputs, making the training process more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    # Generator\n",
    "    # Purpose: to generate an image from a latent vector\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        # z_dim: dimension of the latent vector\n",
    "        # in_channels: number of input channels\n",
    "        # img_channels: number of output channels\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "        print(\"Generator\")\n",
    "\n",
    "        # Begin tranforming the noise vector into an image\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm()\n",
    "        )\n",
    "\n",
    "        # Generate the RGB image output\n",
    "        self.initial_rgb = WSConv2d(in_channels, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # Add to the progressive growing of the GAN \n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([self.initial_rgb])\n",
    "\n",
    "\n",
    "        for i in range(len(factors) - 1):\n",
    "            conv_in_c = int(in_channels * factors[i])       # how much we should scale the input channels by\n",
    "            conv_out_c = int(in_channels * factors[i + 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "    # Smoothly move to the next image resolution\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        # alpha: fade in factor\n",
    "        # upscaled: output of the previous block\n",
    "        # generated: output of the current block\n",
    "\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "    \n",
    "    def forward(self, x, alpha, steps):     # if steps =0 (4x4), if steps = 1 (8x8) ...\n",
    "        # x: latent vector\n",
    "        # alpha: fade in factor\n",
    "        # steps: number of steps to take\n",
    "\n",
    "        out = self.initial(x)\n",
    "        #upscaled = self.initial_rgb(x)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "\n",
    "        # if steps do not equal 0, then we need to go through the progressive growing\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)       # rgb is always one step behind\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Summary of Key Components</h2>**\n",
    "Progressive Structure: The discriminator handles images of increasing resolution by adding blocks dynamically during training.\n",
    "\n",
    "Minibatch Standard Deviation: Enhances robustness against mode collapse by analyzing variability in the batch.\n",
    "\n",
    "Fade-in: Smoothly transitions the network as resolutions increase.\n",
    "\n",
    "Final Block: Produces a single output scalar indicating real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=3):\n",
    "        print(\"Discriminator\")\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # to store the blocks and layers just like in generator\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # create the convolutional blocks. Start from the highest resolution and work our way down\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c, use_pixelnorm=False))\n",
    "            self.rgb_layers.append(WSConv2d(img_channels, conv_in_c, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "        # this is for the 4x4 image resolution\n",
    "        self.initial_rgb = WSConv2d(img_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # block for the 4x4 image resolution\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, 1, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "    \n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    # Improves the discriminators ability to detect subtle differences in the images\n",
    "    def minibatch_std_dev(self, x):\n",
    "        batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])    # calculate the standard deviation of the batch for each channel\n",
    "        return torch.cat([x, batch_statistics], dim=1)     # concatenate the batch statistics to the input tensor. 512 -> 513 channels\n",
    "    \n",
    "    def forward(self, x, alpha, steps):     # if steps =0 (4x4), if steps = 1 (8x8) ...\n",
    "        current_step = len(self.prog_blocks) - steps\n",
    "        out = self.leaky(self.rgb_layers[current_step](x))\n",
    "\n",
    "        # if steps == 0, we are at the 4x4 resolution\n",
    "        if steps == 0:\n",
    "           out = self.minibatch_std_dev(out)\n",
    "           return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        # Downsample the image to the step below to help the discriminator learn. Also computational power\n",
    "        downscaled = self.leaky(self.rgb_layers[current_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[current_step](out))\n",
    "\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        # add one to the current step because we have already done the first right above\n",
    "        #   this for loop is for the remaining steps to just downsample the image\n",
    "        for step in range(current_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std_dev(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if each image size can work with just random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z_DIM = 256\n",
    "# IN_CHANNELS = 256\n",
    "\n",
    "# gen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "# disc = Discriminator(IN_CHANNELS, img_channels=3)\n",
    "\n",
    "# for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "#     num_steps = int(log2(img_size / 4))\n",
    "#     x = torch.randn((1, Z_DIM, 1, 1))\n",
    "#     z = gen(x, alpha=0.5, steps=num_steps)\n",
    "#     assert z.shape == (1, 3, img_size, img_size)\n",
    "#     out = disc(z, alpha=0.5, steps=num_steps)\n",
    "#     assert out.shape == (1, 1)\n",
    "#     print(f\"Passed for image size {img_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Train the Model</h2>\n",
    "    <p>\n",
    "        This section implements the training loop for the generator and discriminator using progressive growing. The <code>get_loader</code> function loads the dataset, resizing images to match the current resolution and normalizing them to [-1, 1]. Batch sizes dynamically adjust for different resolutions, ensuring efficient memory usage. Data is fed into a PyTorch DataLoader for training.\n",
    "    </p>\n",
    "    <p>\n",
    "        The <code>train_fn</code> function handles training at a specific resolution. For the discriminator, real images are compared to generated ones using the Wasserstein loss with gradient penalty, ensuring stable learning. For the generator, the loss measures its ability to create realistic images that fool the discriminator. Alpha blending is used to smoothly transition between resolutions by combining outputs of lower and higher resolution blocks.\n",
    "    </p>\n",
    "    <p>\n",
    "        The <code>main</code> function controls the overall process. It initializes models, optimizers, and gradient scalers for mixed precision training. It begins at the lowest resolution (4x4) and progressively grows to higher resolutions by increasing the step size after training each stage for a predefined number of epochs. During training, visual samples and model checkpoints are saved periodically for monitoring and resuming progress.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator\n",
      "Discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\callahan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "C:\\Users\\callahan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:617: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGiCAYAAAB+sGhNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkFElEQVR4nO3df3BU9aH+8WcDZEMu7IYUkk0gRBQb5Gcg/Np4B2KNpsgwptOZS6nTIBfw6sAdKE4r6bRSsXVrlWqnl8uPcZS2msHSCtxLBRpDgVECSEhGQJopSAl6s0EL7ELUJWTP949+3RpJQoJ7djefvF8zZ8Y9+XzOPhyPPpzdc3IclmVZAgDAYEnxDgAAgN0oOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPFsK7sLFy7ogQcekMvlUlpamhYuXKgrV650OqeoqEgOh6PN8vDDD9sVEQDQSzjs+t2Ys2bNUmNjozZs2KCWlhYtWLBAU6ZMUUVFRYdzioqK9NWvflWrV6+OrEtNTZXL5bIjIgCgl+hrx0ZPnjypXbt26e2339bkyZMlSb/61a9033336dlnn1V2dnaHc1NTU+XxeOyIBQDopWwpu+rqaqWlpUWKTpKKi4uVlJSkQ4cO6Rvf+EaHc1955RW9/PLL8ng8mjNnjn70ox8pNTW1w/GhUEihUCjyOhwO68KFC/rKV74ih8MRnT8QACBmLMvS5cuXlZ2draSk6HzbZkvZ+f1+ZWRktH2jvn2Vnp4uv9/f4bxvf/vbys3NVXZ2tt555x099thjqq+v12uvvdbhHJ/PpyeeeCJq2QEAieHcuXMaNmxYVLbVrbJbuXKlnn766U7HnDx58qbDPPTQQ5F/HjdunLKysnT33Xfr9OnTuu2229qdU15erhUrVkReBwIBDR8+XO6vDJEjSn8jQOdcyeznWEvtnxLvCL2KJT4liqXWcFin3vubBg4cGLVtdqvsHn30UT344IOdjrn11lvl8Xh0/vz5NuuvXbumCxcudOv7uGnTpkmSTp061WHZOZ1OOZ3O69Y7kpIouxiJ1scM6Lo+fdjnsUTZxUc0v4rqVtkNGTJEQ4YMueE4r9erS5cuqaamRgUFBZKkPXv2KBwORwqsK+rq6iRJWVlZ3YkJAEAbtvz18I477tDXv/51LV68WIcPH9Zbb72lpUuX6lvf+lbkSswPPvhAo0aN0uHDhyVJp0+f1pNPPqmamhr97W9/0//8z/+orKxMM2bM0Pjx4+2ICQDoJWz7LOSVV17RqFGjdPfdd+u+++7Tv/7rv2rjxo2Rn7e0tKi+vl4ff/yxJCk5OVlvvPGG7r33Xo0aNUqPPvqovvnNb+p///d/7YoIAOglbLupPF6CwaDcbrfShmTynV2MuLlAJeb+JZULVGKJ7+xiq7U1rPpT7ykQCETtl4rwfykAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Wwvu7Vr1+qWW25RSkqKpk2bpsOHD3c6fsuWLRo1apRSUlI0btw4vf7663ZHBAAYztaye/XVV7VixQqtWrVKR48e1YQJE1RSUqLz58+3O/7AgQOaN2+eFi5cqNraWpWWlqq0tFTHjx+3MyYAwHAOy7IsuzY+bdo0TZkyRf/1X/8lSQqHw8rJydF//ud/auXKldeNnzt3rpqbm7Vjx47IuunTpys/P1/r16/v0nsGg0G53W6lDcmUI4lPaWPBncx+jrV/SU2Jd4RexZIj3hF6ldbWsOpPvadAICCXyxWVbdr2f6mrV6+qpqZGxcXF/3yzpCQVFxerurq63TnV1dVtxktSSUlJh+MlKRQKKRgMtlkAAPg828ruo48+UmtrqzIzM9usz8zMlN/vb3eO3+/v1nhJ8vl8crvdkSUnJ+fLhwcAGKXHf/5UXl6uQCAQWc6dOxfvSACABNPXrg0PHjxYffr0UVNTU5v1TU1N8ng87c7xeDzdGi9JTqdTTqfzywcGABjLtjO75ORkFRQUqKqqKrIuHA6rqqpKXq+33Tler7fNeEmqrKzscDwAAF1h25mdJK1YsULz58/X5MmTNXXqVD3//PNqbm7WggULJEllZWUaOnSofD6fJGnZsmWaOXOm1qxZo9mzZ2vz5s06cuSINm7caGdMAIDhbC27uXPn6sMPP9Tjjz8uv9+v/Px87dq1K3IRSkNDg5I+d3tAYWGhKioq9MMf/lA/+MEPdPvtt2vbtm0aO3asnTEBAIaz9T67eOA+u9jjPrvY4z672OI+u9jqUffZAQCQKCg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxKDsAgPEoOwCA8Sg7AIDxbC+7tWvX6pZbblFKSoqmTZumw4cPdzh206ZNcjgcbZaUlBS7IwIADGdr2b366qtasWKFVq1apaNHj2rChAkqKSnR+fPnO5zjcrnU2NgYWc6ePWtnRABAL2Br2f3iF7/Q4sWLtWDBAo0ePVrr169XamqqXnzxxQ7nOBwOeTyeyJKZmWlnRABAL9DXrg1fvXpVNTU1Ki8vj6xLSkpScXGxqqurO5x35coV5ebmKhwOa9KkSXrqqac0ZsyYDseHQiGFQqHI62AwKElq/aRZDocjCn8S3Ejwap94R+h1+vTrF+8Ivcrg9EHxjtCrXGttjfo2bTuz++ijj9Ta2nrdmVlmZqb8fn+7c/Ly8vTiiy9q+/btevnllxUOh1VYWKj333+/w/fx+Xxyu92RJScnJ6p/DgBAz5dQV2N6vV6VlZUpPz9fM2fO1GuvvaYhQ4Zow4YNHc4pLy9XIBCILOfOnYthYgBAT2Dbx5iDBw9Wnz591NTU1GZ9U1OTPB5Pl7bRr18/TZw4UadOnepwjNPplNPp/FJZAQBms+3MLjk5WQUFBaqqqoqsC4fDqqqqktfr7dI2WltbdezYMWVlZdkVEwDQC9h2ZidJK1as0Pz58zV58mRNnTpVzz//vJqbm7VgwQJJUllZmYYOHSqfzydJWr16taZPn66RI0fq0qVLeuaZZ3T27FktWrTIzpgAAMPZWnZz587Vhx9+qMcff1x+v1/5+fnatWtX5KKVhoYGJSX98+Ty4sWLWrx4sfx+vwYNGqSCggIdOHBAo0ePtjMmAMBwDsuyrHiHiKZgMCi3262BAwZw60GM9O3LrQexNmhQWrwj9CrcehBb11pbVVP7jgKBgFwuV1S2mVBXYwIAYAfKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8W8tu//79mjNnjrKzs+VwOLRt27Ybztm7d68mTZokp9OpkSNHatOmTXZGBAD0AraWXXNzsyZMmKC1a9d2afyZM2c0e/Zs3XXXXaqrq9Py5cu1aNEi7d69286YAADD9bVz47NmzdKsWbO6PH79+vUaMWKE1qxZI0m644479Oabb+q5555TSUlJu3NCoZBCoVDkdTAY/HKhAQDGSajv7Kqrq1VcXNxmXUlJiaqrqzuc4/P55Ha7I0tOTo7dMQEAPUxClZ3f71dmZmabdZmZmQoGg/rkk0/anVNeXq5AIBBZzp07F4uoAIAexNaPMWPB6XTK6XTGOwYAIIEl1Jmdx+NRU1NTm3VNTU1yuVzq379/nFIBAHq6hCo7r9erqqqqNusqKyvl9XrjlAgAYAJby+7KlSuqq6tTXV2dpH/cWlBXV6eGhgZJ//i+raysLDL+4Ycf1nvvvafvf//7+stf/qL//u//1u9+9zt997vftTMmAMBwtpbdkSNHNHHiRE2cOFGStGLFCk2cOFGPP/64JKmxsTFSfJI0YsQI/fGPf1RlZaUmTJigNWvW6IUXXujwtgMAALrCYVmWFe8Q0RQMBuV2uzVwwAA5HI54x+kV+vbtE+8Ivc6gQWnxjtCrDE4fFO8Ivcq11lbV1L6jQCAgl8sVlW0m1Hd2AADYgbIDABiPsgMAGI+yAwAYj7IDABiPsgMAGI+yAwAYj7IDABiPsgMAGI+yAwAYj7IDABiPsgMAGI+yAwAYj7IDABiPsgMAGI+yAwAYj7IDABiPsgMAGI+yAwAYj7IDABiPsgMAGI+yAwAYj7IDABiPsgMAGI+yAwAYj7IDABiPsgMAGI+yAwAYj7IDABiPsgMAGI+yAwAYj7IDABiPsgMAGI+yAwAYj7IDABjP1rLbv3+/5syZo+zsbDkcDm3btq3T8Xv37pXD4bhu8fv9dsYEABjO1rJrbm7WhAkTtHbt2m7Nq6+vV2NjY2TJyMiwKSEAoDfoa+fGZ82apVmzZnV7XkZGhtLS0ro0NhQKKRQKRV4Hg8Fuvx8AwGy2lt3Nys/PVygU0tixY/XjH/9Yd955Z4djfT6fnnjiievWp3/lK0pK4ivJWGj59ON4R+h10lyueEfoVbKzh8U7Qq/S0tIi6Z2objOh2iArK0vr16/XH/7wB/3hD39QTk6OioqKdPTo0Q7nlJeXKxAIRJZz587FMDEAoCdIqDO7vLw85eXlRV4XFhbq9OnTeu655/Tb3/623TlOp1NOpzNWEQEAPVBCndm1Z+rUqTp16lS8YwAAerCEL7u6ujplZWXFOwYAoAez9WPMK1eutDkrO3PmjOrq6pSenq7hw4ervLxcH3zwgX7zm99Ikp5//nmNGDFCY8aM0aeffqoXXnhBe/bs0Z/+9Cc7YwIADGdr2R05ckR33XVX5PWKFSskSfPnz9emTZvU2NiohoaGyM+vXr2qRx99VB988IFSU1M1fvx4vfHGG222AQBAdzksy7LiHSKagsGg3G63cnNzufUgRrj1IPYyMzPjHaFXGT48N94RepWWlhbt2LlLgUBArijdZkMbAACMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxna9n5fD5NmTJFAwcOVEZGhkpLS1VfX3/DeVu2bNGoUaOUkpKicePG6fXXX7czJgDAcLaW3b59+7RkyRIdPHhQlZWVamlp0b333qvm5uYO5xw4cEDz5s3TwoULVVtbq9LSUpWWlur48eN2RgUAGMxhWZYVqzf78MMPlZGRoX379mnGjBntjpk7d66am5u1Y8eOyLrp06crPz9f69evv+F7BINBud1u5ebmKimJT2ljoeXTj+MdodfJzMyMd4ReZfjw3HhH6FVaWlq0Y+cuBQIBuVyuqGwzpm0QCAQkSenp6R2Oqa6uVnFxcZt1JSUlqq6ubnd8KBRSMBhsswAA8HkxK7twOKzly5frzjvv1NixYzsc5/f7r/tba2Zmpvx+f7vjfT6f3G53ZMnJyYlqbgBAzxezsluyZImOHz+uzZs3R3W75eXlCgQCkeXcuXNR3T4AoOfrG4s3Wbp0qXbs2KH9+/dr2LBhnY71eDxqampqs66pqUkej6fd8U6nU06nM2pZAQDmsfXMzrIsLV26VFu3btWePXs0YsSIG87xer2qqqpqs66yslJer9eumAAAw9l6ZrdkyRJVVFRo+/btGjhwYOR7N7fbrf79+0uSysrKNHToUPl8PknSsmXLNHPmTK1Zs0azZ8/W5s2bdeTIEW3cuNHOqAAAg9l6Zrdu3ToFAgEVFRUpKysrsrz66quRMQ0NDWpsbIy8LiwsVEVFhTZu3KgJEybo97//vbZt29bpRS0AAHQmpvfZxQL32cUe99nFHvfZxRb32cVWj7/PDgCAeKDsAADGo+wAAMaj7AAAxqPsAADGo+wAAMaj7AAAxqPsAADGo+wAAMaj7AAAxqPsAADGo+wAAMaj7AAAxqPsAADGo+wAAMaj7AAAxqPsAADGo+wAAMaj7AAAxqPsAADGo+wAAMaj7AAAxqPsAADGo+wAAMaj7AAAxqPsAADGo+wAAMaj7AAAxqPsAADGo+wAAMaj7AAAxqPsAADGo+wAAMaj7AAAxqPsAADGs7XsfD6fpkyZooEDByojI0OlpaWqr6/vdM6mTZvkcDjaLCkpKXbGBAAYztay27dvn5YsWaKDBw+qsrJSLS0tuvfee9Xc3NzpPJfLpcbGxshy9uxZO2MCAAzX186N79q1q83rTZs2KSMjQzU1NZoxY0aH8xwOhzwej53RAAC9iK1l90WBQECSlJ6e3um4K1euKDc3V+FwWJMmTdJTTz2lMWPGtDs2FAopFApFXgeDQUlSS0tISUl8JRkLg93ueEfodZwWx3YsfXTxYrwj9CrXrl2L+jZj9l9MOBzW8uXLdeedd2rs2LEdjsvLy9OLL76o7du36+WXX1Y4HFZhYaHef//9dsf7fD653e7IkpOTY9cfAQDQQzksy7Ji8UaPPPKIdu7cqTfffFPDhg3r8ryWlhbdcccdmjdvnp588snrft7emV1OTo6ysz2c2cXI4AED4h2h10l1psY7Qq/Sx8UxHkvXrl1TdfVBBQIBuVyuqGwzJh9jLl26VDt27ND+/fu7VXSS1K9fP02cOFGnTp1q9+dOp1NOpzMaMQEAhrL11MeyLC1dulRbt27Vnj17NGLEiG5vo7W1VceOHVNWVpYNCQEAvYGtZ3ZLlixRRUWFtm/froEDB8rv90uS3G63+vfvL0kqKyvT0KFD5fP5JEmrV6/W9OnTNXLkSF26dEnPPPOMzp49q0WLFtkZFQBgMFvLbt26dZKkoqKiNutfeuklPfjgg5KkhoaGNt+tXbx4UYsXL5bf79egQYNUUFCgAwcOaPTo0XZGBQAYLGYXqMRKMBiU2+3mApUY4gKV2OMCldjiApXYsuMCFdoAAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDzKDgBgPMoOAGA8yg4AYDxby27dunUaP368XC6XXC6XvF6vdu7c2emcLVu2aNSoUUpJSdG4ceP0+uuv2xkRANAL2Fp2w4YN089+9jPV1NToyJEj+trXvqb7779fJ06caHf8gQMHNG/ePC1cuFC1tbUqLS1VaWmpjh8/bmdMAIDhHJZlWbF8w/T0dD3zzDNauHDhdT+bO3eumpubtWPHjsi66dOnKz8/X+vXr+/S9oPBoNxut7KzPUpK4lPaWBg8YEC8I/Q6qc7UeEfoVfq4OMZj6dq1a6quPqhAICCXyxWVbcasDVpbW7V582Y1NzfL6/W2O6a6ulrFxcVt1pWUlKi6urrD7YZCIQWDwTYLAACfZ3vZHTt2TAMGDJDT6dTDDz+srVu3avTo0e2O9fv9yszMbLMuMzNTfr+/w+37fD653e7IkpOTE9X8AICez/ayy8vLU11dnQ4dOqRHHnlE8+fP17vvvhu17ZeXlysQCESWc+fORW3bAAAz9LX7DZKTkzVy5EhJUkFBgd5++2398pe/1IYNG64b6/F41NTU1GZdU1OTPB5Ph9t3Op1yOp3RDQ0AMErMr+AIh8MKhULt/szr9aqqqqrNusrKyg6/4wMAoCtsPbMrLy/XrFmzNHz4cF2+fFkVFRXau3evdu/eLUkqKyvT0KFD5fP5JEnLli3TzJkztWbNGs2ePVubN2/WkSNHtHHjRjtjAgAMZ2vZnT9/XmVlZWpsbJTb7db48eO1e/du3XPPPZKkhoaGNrcHFBYWqqKiQj/84Q/1gx/8QLfffru2bdumsWPH2hkTAGC4mN9nZzfus4s97rOLPe6ziy3us4utHn2fHQAA8ULZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxH2QEAjEfZAQCMR9kBAIxna9mtW7dO48ePl8vlksvlktfr1c6dOzscv2nTJjkcjjZLSkqKnREBAL1AXzs3PmzYMP3sZz/T7bffLsuy9Otf/1r333+/amtrNWbMmHbnuFwu1dfXR147HA47IwIAegFby27OnDltXv/0pz/VunXrdPDgwQ7LzuFwyOPxdPk9QqGQQqFQ5HUgEJAkhcPhm0iMm9Ha2hrvCL3ONfZ5TFnXrsU7Qq9y7f/vb8uyorZNW8vu81pbW7VlyxY1NzfL6/V2OO7KlSvKzc1VOBzWpEmT9NRTT3VYjJLk8/n0xBNPXLfe7z8fldy4sf+LdwAARvr73/8ut9sdlW05rGhWZzuOHTsmr9erTz/9VAMGDFBFRYXuu+++dsdWV1frr3/9q8aPH69AIKBnn31W+/fv14kTJzRs2LB253zxzO7SpUvKzc1VQ0ND1HZSLASDQeXk5OjcuXNyuVzxjtMtPTU7uWOL3LHXU7MHAgENHz5cFy9eVFpaWlS2afuZXV5enurq6hQIBPT73/9e8+fP1759+zR69Ojrxnq93jZnfYWFhbrjjju0YcMGPfnkk+1u3+l0yul0Xrfe7Xb3qH+5n/nsYp6eqKdmJ3dskTv2emr2pKToXUNpe9klJydr5MiRkqSCggK9/fbb+uUvf6kNGzbccG6/fv00ceJEnTp1yu6YAACDxfw+u3A43OZjx860trbq2LFjysrKsjkVAMBktp7ZlZeXa9asWRo+fLguX76siooK7d27V7t375YklZWVaejQofL5fJKk1atXa/r06Ro5cqQuXbqkZ555RmfPntWiRYu6/J5Op1OrVq1q96PNRNZTc0s9Nzu5Y4vcsddTs9uR29YLVBYuXKiqqio1NjbK7XZr/Pjxeuyxx3TPPfdIkoqKinTLLbdo06ZNkqTvfve7eu211+T3+zVo0CAVFBToJz/5iSZOnGhXRABAL2D71ZgAAMQbvxsTAGA8yg4AYDzKDgBgPMoOAGA8I8ruwoULeuCBB+RyuZSWlqaFCxfqypUrnc4pKiq67nFCDz/8sK05165dq1tuuUUpKSmaNm2aDh8+3On4LVu2aNSoUUpJSdG4ceP0+uuv25qvM93JngiPatq/f7/mzJmj7OxsORwObdu27YZz9u7dq0mTJsnpdGrkyJGRq4RjrbvZ9+7de93+djgc8vv9sQmsf/yO2ilTpmjgwIHKyMhQaWlpm6eXdCTex/jN5E6E41vq/iPUpPjvbyl+j34zouweeOABnThxQpWVldqxY4f279+vhx566IbzFi9erMbGxsjy85//3LaMr776qlasWKFVq1bp6NGjmjBhgkpKSnT+fPu/sPrAgQOaN2+eFi5cqNraWpWWlqq0tFTHjx+3LWNHuptd+sevJ/r8vj179mwME0vNzc2aMGGC1q5d26XxZ86c0ezZs3XXXXeprq5Oy5cv16JFiyL3hMZSd7N/pr6+vs0+z8jIsCnh9fbt26clS5bo4MGDqqysVEtLi+699141Nzd3OCcRjvGbyS3F//iW/vkItZqaGh05ckRf+9rXdP/99+vEiRPtjk+E/X0zuaUo7W+rh3v33XctSdbbb78dWbdz507L4XBYH3zwQYfzZs6caS1btiwGCf9h6tSp1pIlSyKvW1tbrezsbMvn87U7/t/+7d+s2bNnt1k3bdo06z/+4z9szdme7mZ/6aWXLLfbHaN0NybJ2rp1a6djvv/971tjxoxps27u3LlWSUmJjclurCvZ//znP1uSrIsXL8YkU1ecP3/ekmTt27evwzGJdIx/piu5E+34/rxBgwZZL7zwQrs/S8T9/ZnOckdrf/f4M7vq6mqlpaVp8uTJkXXFxcVKSkrSoUOHOp37yiuvaPDgwRo7dqzKy8v18ccf25Lx6tWrqqmpUXFxcWRdUlKSiouLVV1d3e6c6urqNuMlqaSkpMPxdrmZ7NI/H9WUk5Nzw7+1JYJE2d9fRn5+vrKysnTPPfforbfeimuWz54rmZ6e3uGYRNznXcktJd7x3draqs2bN3f6CLVE3N9dyS1FZ3/H7Hl2dvH7/dd9XNO3b1+lp6d3+p3Ft7/9beXm5io7O1vvvPOOHnvsMdXX1+u1116LesaPPvpIra2tyszMbLM+MzNTf/nLX9qd4/f72x0fy+9hpJvLnpeXpxdffLHNo5oKCws7fVRTvHW0v4PBoD755BP1798/TsluLCsrS+vXr9fkyZMVCoX0wgsvqKioSIcOHdKkSZNiniccDmv58uW68847NXbs2A7HJcox/pmu5k6k4/uLj1DbunVru0+UkRJrf3cnd7T2d8KW3cqVK/X00093OubkyZM3vf3Pf6c3btw4ZWVl6e6779bp06d122233fR2cXOPasLNy8vLU15eXuR1YWGhTp8+reeee06//e1vY55nyZIlOn78uN58882Yv/eX0dXciXR8d+cRaonE7ke/tSdhy+7RRx/Vgw8+2OmYW2+9VR6P57oLJa5du6YLFy7I4/F0+f2mTZsmSTp16lTUy27w4MHq06ePmpqa2qxvamrqMKPH4+nWeLvcTPYv6gmPaupof7tcroQ+q+vI1KlT41I2S5cujVwkdqO/dSfKMS51L/cXxfP47s4j1BJpf8fj0W8J+53dkCFDNGrUqE6X5ORkeb1eXbp0STU1NZG5e/bsUTgcjhRYV9TV1UmSLY8TSk5OVkFBgaqqqiLrwuGwqqqqOvyc2uv1thkvSZWVlZ1+rm2Hm8n+RT3hUU2Jsr+jpa6uLqb727IsLV26VFu3btWePXs0YsSIG85JhH1+M7m/KJGO784eoZYI+7sjMXn025e+xCUBfP3rX7cmTpxoHTp0yHrzzTet22+/3Zo3b17k5++//76Vl5dnHTp0yLIsyzp16pS1evVq68iRI9aZM2es7du3W7feeqs1Y8YM2zJu3rzZcjqd1qZNm6x3333Xeuihh6y0tDTL7/dblmVZ3/nOd6yVK1dGxr/11ltW3759rWeffdY6efKktWrVKqtfv37WsWPHbMsYrexPPPGEtXv3buv06dNWTU2N9a1vfctKSUmxTpw4EbPMly9ftmpra63a2lpLkvWLX/zCqq2ttc6ePWtZlmWtXLnS+s53vhMZ/95771mpqanW9773PevkyZPW2rVrrT59+li7du2KWeabzf7cc89Z27Zts/76179ax44ds5YtW2YlJSVZb7zxRswyP/LII5bb7bb27t1rNTY2RpaPP/44MiYRj/GbyZ0Ix7dl/eM42Ldvn3XmzBnrnXfesVauXGk5HA7rT3/6U7u5E2F/30zuaO1vI8ru73//uzVv3jxrwIABlsvlshYsWGBdvnw58vMzZ85Ykqw///nPlmVZVkNDgzVjxgwrPT3dcjqd1siRI63vfe97ViAQsDXnr371K2v48OFWcnKyNXXqVOvgwYORn82cOdOaP39+m/G/+93vrK9+9atWcnKyNWbMGOuPf/yjrfk6053sy5cvj4zNzMy07rvvPuvo0aMxzfvZ5fhfXD7LOX/+fGvmzJnXzcnPz7eSk5OtW2+91XrppZdimvnzObqT/emnn7Zuu+02KyUlxUpPT7eKioqsPXv2xDRze3kltdmHiXiM30zuRDi+Lcuy/v3f/93Kzc21kpOTrSFDhlh33313pDDay21Z8d/fltX93NHa3zziBwBgvIT9zg4AgGih7AAAxqPsAADGo+wAAMaj7AAAxqPsAADGo+wAAMaj7AAAxqPsAADGo+wAAMaj7AAAxvt/q0IAioDTB3IAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Size: 4\n",
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0, Alpha: 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1141/1141 [15:17<00:00,  1.24it/s, loss_disc=0.0477, loss_gen=0.221]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0, Alpha: 0.39999999999999586: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1141/1141 [15:38<00:00,  1.22it/s, loss_disc=-0.0515, loss_gen=0.115]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0, Alpha: 0.7999999999999766: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1141/1141 [15:47<00:00,  1.20it/s, loss_disc=0.028, loss_gen=-0.0696]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0, Alpha: 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1141/1141 [15:34<00:00,  1.22it/s, loss_disc=0.144, loss_gen=0.058]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0, Alpha: 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1141/1141 [15:18<00:00,  1.24it/s, loss_disc=0.0462, loss_gen=-0.0415]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "torch.Size([3, 8, 8])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Size: 8\n",
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 1, Alpha: 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2281/2281 [1:15:34<00:00,  1.99s/it, loss_disc=-0.0789, loss_gen=-0.214]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 1, Alpha: 0.3999999999999883: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2281/2281 [1:14:23<00:00,  1.96s/it, loss_disc=-0.0142, loss_gen=0.806]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 1, Alpha: 0.7999999999999691: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2281/2281 [1:14:59<00:00,  1.97s/it, loss_disc=0.117, loss_gen=0.398]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 1, Alpha: 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2281/2281 [1:15:32<00:00,  1.99s/it, loss_disc=-0.343, loss_gen=0.755]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 1, Alpha: 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2281/2281 [1:14:19<00:00,  1.95s/it, loss_disc=-0.123, loss_gen=0.453]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "torch.Size([3, 16, 16])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Size: 16\n",
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 2, Alpha: 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2281/2281 [5:04:43<00:00,  8.02s/it, loss_disc=-0.567, loss_gen=-1.21]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 2, Alpha: 0.3999999999999883:   7%|â–‹         | 170/2281 [24:10<4:40:52,  7.98s/it, loss_disc=-0.645, loss_gen=3.3]    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 135\u001b[0m\n\u001b[0;32m    131\u001b[0m         step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 122\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_disc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_gen\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m# If save model is true, save the model every epoch\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Save_Model:\n",
      "Cell \u001b[1;32mIn[12], line 59\u001b[0m, in \u001b[0;36mtrain_fn\u001b[1;34m(disc, gen, loader, dataset, step, alpha, gen_opt, disc_opt, scaler_disc, scaler_gen)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Train Generator: max E[critic(gen_fake)]\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 59\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mdisc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     loss_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(output)\n\u001b[0;32m     62\u001b[0m gen_opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 58\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x, alpha, steps)\u001b[0m\n\u001b[0;32m     55\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_pool(out)\n\u001b[0;32m     57\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminibatch_std_dev(out)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m, in \u001b[0;36mWSConv2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.backends.cudnn.benchmark = True # Improves performance if input sizes don't change\n",
    "\n",
    "def get_loader(image_size):\n",
    "    # Purpose: to load the dataset and apply data augmentation to the images\n",
    "    # Inputs: image_size: the size of the images to be loaded. \n",
    "    #           This will change as training continues\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                [0.5 for _ in range(Channel)],\n",
    "                [0.5 for _ in range(Channel)],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    # get the batch size for the current image size from the batch sizes list\n",
    "    batch_size = Batch_Sizes[int(log2(image_size / 4))]\n",
    "    \n",
    "    # load the dataset\n",
    "    dataset = datasets.ImageFolder(root=image_dataset, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return loader, dataset\n",
    "\n",
    "def train_fn(disc, gen, loader, dataset, step, alpha, gen_opt, disc_opt, scaler_disc, scaler_gen):\n",
    "    # Purpose: to train the generator and discriminator\n",
    "    # Inputs:\n",
    "    #       disc: the discriminator model\n",
    "    #       gen: the generator model\n",
    "    #       loader: the data loader\n",
    "    #       dataset: the dataset\n",
    "    #       step: the current step in the progressive growing\n",
    "    #       alpha: the fade in factor\n",
    "    #       gen_opt: the generator optimizer\n",
    "    #       disc_opt: the discriminator optimizer\n",
    "    #       scaler_disc: the gradient scaler for the discriminator\n",
    "    #       scaler_gen: the gradient scaler for the generator\n",
    "\n",
    "    loop = tqdm(loader, leave=True, desc=f\"Step: {step}, Alpha: {alpha}\", disable=False)\n",
    "\n",
    "    # iterate through the data loader for the number of epochs.\n",
    "    #   We are training the discriminator and generator at the same time\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        # Train Discriminator: max E[critic(real)] - E[critic(fake)]\n",
    "        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cpu'):\n",
    "            fake = gen(noise, alpha, step)\n",
    "            disc_real = disc(real, alpha, step)\n",
    "            disc_fake = disc(fake.detach(), alpha, step)\n",
    "            gp = gradient_penalty(disc, real, fake, alpha, step)\n",
    "\n",
    "            loss_disc = (\n",
    "                -(torch.mean(disc_real) - torch.mean(disc_fake)) \n",
    "                + LAMDA_GP * gp\n",
    "                + (0.001 * torch.mean(disc_real ** 2))  # Ensure critic doesn't get too far from 0\n",
    "            )\n",
    "\n",
    "        disc_opt.zero_grad()\n",
    "        scaler_disc.scale(loss_disc).backward()\n",
    "        scaler_disc.step(disc_opt)\n",
    "        scaler_disc.update()\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake)]\n",
    "        with torch.amp.autocast(device_type='cpu'):\n",
    "            output = disc(fake, alpha, step)\n",
    "            loss_gen = -torch.mean(output)\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(gen_opt)\n",
    "        scaler_gen.update()\n",
    "\n",
    "        # Update alpha for fade-in\n",
    "        alpha += cur_batch_size / (len(dataset) * 0.5 * PROGRESSIVE_EPOCHS[step])\n",
    "        alpha = min(alpha, 1)\n",
    "\n",
    "        if batch_idx % 1100 == 0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes = gen(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n",
    "                save_image(fixed_fakes, f\"saved_images/img_{batch_idx}.png\")\n",
    "        \n",
    "        loop.update(1)  # Only updates once per batch but keeps silent otherwise\n",
    "        loop.set_postfix(loss_disc=loss_disc.item(), loss_gen=loss_gen.item())\n",
    "\n",
    "    loop.close()\n",
    "    return alpha\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    gen = Generator(Z_DIM, IN_CHANNELS, Channel)\n",
    "    disc = Discriminator(IN_CHANNELS, Channel)\n",
    "\n",
    "    # initialize the optimizers\n",
    "    gen_opt = torch.optim.Adam(gen.parameters(), lr=Learning_Rate, betas=(0.0, 0.99))   # betas are the exponential decay rates for the first and second moment estimates\n",
    "    disc_opt = torch.optim.Adam(disc.parameters(), lr=Learning_Rate, betas=(0.0, 0.99))\n",
    "    scaler_disc = torch.amp.GradScaler()\n",
    "    scaler_gen = torch.amp.GradScaler()\n",
    "\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "    step = int(log2(START_TRAIN_AT_IMG_SIZE / 4)) # is going to be 0 when we start training at 4x4\n",
    "\n",
    "    # Get the number of epochs for the current step\n",
    "    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "        alpha = 0\n",
    "        loader, dataset = get_loader(4 * 2 ** step)\n",
    "\n",
    "        # Print a sample image from the dataset that we are about to train on\n",
    "        images, labels = next(iter(loader))\n",
    "        # Denormalize the image (from [-1, 1] to [0, 1])\n",
    "        image = images[0]  # Select the first image in the batch\n",
    "        print(image.shape)\n",
    "        image = image * 0.5 + 0.5  # Reverse normalization\n",
    "        image_np = image.permute(1, 2, 0).numpy()  # Change shape to H x W x C\n",
    "        # Display the image\n",
    "        plt.imshow(image_np)\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Image Size: {4 * 2 ** step}\")\n",
    "\n",
    "        # Train the models\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "            alpha = train_fn(\n",
    "                disc, gen, loader, dataset, step, alpha, gen_opt, disc_opt, scaler_disc, scaler_gen\n",
    "            )\n",
    "\n",
    "            # If save model is true, save the model every epoch\n",
    "            if Save_Model:\n",
    "                save_checkpoint(gen, gen_opt, filename=CHECKPOINT_GEN)\n",
    "                save_checkpoint(disc, disc_opt, filename=CHECKPOINT_DISC)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Test the Model</h2>\n",
    "    <p>\n",
    "        4x4 test images that the discriminator generated\n",
    "    </p>\n",
    "    <p>\n",
    "        <img src=\"results/4x4 img0.JPG\">\n",
    "    </p>\n",
    "    <p>\n",
    "        <img src=\"results/4x4 img1.JPG\">\n",
    "    </p>\n",
    "    <p>\n",
    "        8x8 test images that the discriminator generated\n",
    "    </p>\n",
    "    <p>\n",
    "        <img src=\"results/8x8 img0.PNG\">\n",
    "    </p>\n",
    "    <p>\n",
    "        16x16 test images that the discriminator generated\n",
    "    </p>\n",
    "    <p>\n",
    "        <img src=\"results/16x16 img0.JPG\">\n",
    "    </p>\n",
    "    <p>\n",
    "        <img src=\"results/16x16 img1.JPG\">\n",
    "    </p>\n",
    "    <p>\n",
    "        <img src=\"results/16x16 img2.JPG\">\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Analyze the Results</h2>\n",
    "    <p>\n",
    "        First of all from the eye test we can see that in the 16x16 that some semblance of dogs are starting to appear. But we should also examine what statistics each epoch was returning.\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>For the 4x4:</b> The training results indicate steady progress during the 4x4 resolution stage of a progressive growing GAN. Over five epochs, the alpha value transitions smoothly from 0 to 1, signaling a successful blending of lower and higher-resolution features. The generator and discriminator losses fluctuate, as is typical in GAN training, but generally trend toward stabilization. Early epochs show the discriminator dominating, with higher generator losses indicating difficulty in producing convincing samples. By later epochs, the generator recovers, evidenced by losses closer to zero and occasional negative values, suggesting improved sample quality. \n",
    "    </p>\n",
    "    <p>\n",
    "        <b>For the 8x8:</b>During the 8x8 resolution phase of training, the GAN shows continued progress, with alpha values smoothly transitioning from 0 to 1 over five epochs. The losses of both the discriminator and generator display notable fluctuations, a hallmark of adversarial training. Initially, the generator struggles, as seen in negative loss values, but recovers in later epochs with positive losses, indicating better-quality outputs. The discriminator's loss varies significantly, suggesting a dynamic balance in the adversarial process. Each epoch takes approximately 75 minutes due to the increased complexity of handling larger images.\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>For the 16x16:</b>This took way too long to run so only one epoch was computationally afforded to run. For what we have, the training process significantly slows down due to the increased complexity of handling larger images. In the first epoch, the training takes over five hours, with the discriminator showing a negative loss value (-0.567) and the generator struggling even more with a loss of -1.21. This suggests that the generator's output is still far from ideal, and the discriminator is pushing it harder. In the second epoch, the losses start to improve, with the generator showing a sharp increase in its loss (+3.3), indicating that it is starting to generate more convincing images.\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Conclusion</h2>\n",
    "    <p>\n",
    "        In conclusion, this project serves more as a research exercise and proof of concept rather than yielding optimal results at this stage. While the model demonstrated gradual progress, the computational resources available were limiting, especially in terms of training at higher resolutions. Utilizing more powerful hardware, such as GPUs in the VR lab, could allow for longer training times, more epochs, and improved results at higher image resolutions, which is essential for generating more realistic outputs.\n",
    "    </p>\n",
    "    <p>\n",
    "        Throughout the training, the discriminator and generator exhibited a cyclical pattern of learning. As expected, when the discriminator became more powerful, the generator had to catch up, and vice versa. Initially, the generator struggled to produce convincing images, but as the training progressed, it started to generate more plausible outputs. The fade-in mechanism, which helps smooth the transition between different image resolutions, also played a crucial role in stabilizing the training process.\n",
    "    </p>\n",
    "    <p>\n",
    "        The reason we stopped the epochs early was because we ran out of time to submit as the higher the resolution the higher the computation power due to the amount of data being passed into the model and also the amount of layers that we have already built upon.\n",
    "    </p>\n",
    "    <p>\n",
    "        Despite the challenges, the project shows considerable promise, and with continued tuning and access to more computational resources, more consistent and high-quality image generation is achievable. Future experiments could explore additional architectural tweaks, data augmentation strategies, and extended training epochs for improved outcomes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>References</h2>\n",
    "    <p>\n",
    "        Kaggle dataset: https://www.kaggle.com/datasets/wutheringwang/dog-face-recognition\n",
    "    </p>\n",
    "    <p>\n",
    "        ProGAN tutorial: https://www.youtube.com/watch?v=nkQHASviYac\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
