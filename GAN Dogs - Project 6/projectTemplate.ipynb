{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .title {\n",
    "        font-family: 'Georgia', serif;\n",
    "        text-align: center;\n",
    "        margin-top: 40px;\n",
    "    }\n",
    "\n",
    "    .title h1 {\n",
    "        font-weight: 700;\n",
    "        font-size: 48px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        line-height: 1.2;\n",
    "    }\n",
    "\n",
    "    .title p {\n",
    "        font-size: 18px;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "\n",
    "    .title hr {\n",
    "        border: none;\n",
    "        border-top: 1px solid #BBBBBB; /* Light grey for light mode */\n",
    "        width: 60%;\n",
    "        margin: 20px auto;\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .title h1 {\n",
    "            color: #F5F5F5; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .title p {\n",
    "            color: #BBBBBB; /* Lighter grey for dark mode */\n",
    "        }\n",
    "\n",
    "        .title hr {\n",
    "            border-top: 1px solid #555555; /* Dark grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"title\">\n",
    "    <h1>Generative Adversarial Network - Dogs</h1>\n",
    "    <p>By: Trevor Pope and Micheal Callahan</p>\n",
    "    <hr>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Problem Statement</h2>\n",
    "    <p>\n",
    "        ...\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Algorithm</h2>\n",
    "    <p>\n",
    "        ...\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Data Initialization</h2>\n",
    "    <p>\n",
    "        ...\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Data Preprocessing</h2>\n",
    "    <p>\n",
    "        ...\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18247"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first image in the dataset\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "all_images = []\n",
    "\n",
    "for folder in os.listdir('data/train'):\n",
    "    for image in os.listdir('data/train/' + folder):\n",
    "        img = cv.imread('data/train/' + folder + '/' + image)\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        img = cv.resize(img, (512, 512))\n",
    "\n",
    "        # add the image to all_images\n",
    "        all_images.append(img)\n",
    "\n",
    "len(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(all_images):\n",
    "    cv.imwrite('data/all_images/' + str(i) + '.jpg', cv.cvtColor(img, cv.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import log2\n",
    "\n",
    "START_TRAIN_AT_IMG_SIZE = 4     # This is the size (4 x 4) of the images that the training will start at\n",
    "dataset = 'data/all_images'\n",
    "CHECKPOINT_GEN = \"models/generator.pth\" # The path to the generator checkpoint that will be used for inference\n",
    "CHECKPOINT_DISC = \"models/discriminator.pth\" # The path to the discriminator checkpoint that will be used for inference\n",
    "Save_Model = True\n",
    "Load_Model = False\n",
    "Learning_Rate = 1e-3\n",
    "Batch_Sizes = [16, 16, 16, 16, 16, 16, 16, 8, 4]\n",
    "Image_Size = 512    # The size of the images that the model was trained on\n",
    "Channel = 3\n",
    "Z_DIM = 256        # The dimension of the noise vector, used in the generator\n",
    "IN_CHANNELS = 256   # The number of channels in input images\n",
    "LAMDA_GP = 10       # Gradient penalty lambda hyperparameter\n",
    "NUM_STEPS = int(log2(Image_Size/4)) + 1   # Number of steps it took to get to that image size (4x4)\n",
    "\n",
    "PROGRESSIVE_EPOCHS = [10] * len(Batch_Sizes) # Number of epochs to train at each image size\n",
    "FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1) # The fixed noise used during inference for evaluation\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# Print losses occasionally and print to tensorboard\n",
    "def plot_to_tensorboard(\n",
    "    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n",
    "):\n",
    "    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # take out (up to) 8 examples to plot\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
    "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
    "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n",
    "\n",
    "\n",
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "# def generate_examples(gen, steps, truncation=0.7, n=100):\n",
    "#     \"\"\"\n",
    "#     Tried using truncation trick here but not sure it actually helped anything, you can\n",
    "#     remove it if you like and just sample from torch.randn\n",
    "#     \"\"\"\n",
    "#     gen.eval()\n",
    "#     alpha = 1.0\n",
    "#     for i in range(n):\n",
    "#         with torch.no_grad():\n",
    "#             noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, config.Z_DIM, 1, 1)), device=config.DEVICE, dtype=torch.float32)\n",
    "#             img = gen(noise, alpha, steps)\n",
    "#             save_image(img*0.5+0.5, f\"saved_examples/img_{i}.png\")\n",
    "#     gen.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Build the Model</h2>\n",
    "    <p>\n",
    "        ...\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import log2\n",
    "\n",
    "factors = [1,1,1,1, 1/2, 1/4, 1/8, 1/16, 1/32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    # Weight scaled Conv2d\n",
    "    # Purpose: to scale the weights of the convolutional layer by a constant factor to prevent the weights from becoming too large\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "\n",
    "        # in_channels: number of input channels\n",
    "        # out_channels: number of output channels\n",
    "        # kernel_size: size of the kernel\n",
    "        # stride: stride of the convolution\n",
    "        # padding: padding of the convolution\n",
    "        # gain: gain factor to scale the weights by\n",
    "\n",
    "        super(WSConv2d, self).__init__()\n",
    "        \n",
    "        # create a convolutional layer\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "        # calculate the scale factor\n",
    "        self.scale = (gain / (in_channels * kernel_size ** 2)) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "        \n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    # Pixelwise feature vector normalization\n",
    "    # Purpose: to normalize the feature vectors of the input image\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = 1e-8\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    # Convolutional block\n",
    "    # Purpose: to apply two convolutional layers with leaky ReLU activation functions\n",
    "    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n",
    "        # in_channels: number of input channels\n",
    "        # out_channels: number of output channels\n",
    "        # use_pixelnorm: flag to determine if pixel normalization should be applied\n",
    "        \n",
    "        super().__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "        self.use_pixelnorm = use_pixelnorm\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pixelnorm else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pixelnorm else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # Generator\n",
    "    # Purpose: to generate an image from a latent vector\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        # z_dim: dimension of the latent vector\n",
    "        # in_channels: number of input channels\n",
    "        # img_channels: number of output channels\n",
    "\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm()\n",
    "        )\n",
    "\n",
    "        self.initial_rgb = WSConv2d(in_channels, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList([self.initial_rgb])\n",
    "\n",
    "        for i in range(len(factors) - 1):\n",
    "            conv_in_c = int(in_channels * factors[i])       # how much we should scale the input channels by\n",
    "            conv_out_c = int(in_channels * factors[i + 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        # alpha: fade in factor\n",
    "        # upscaled: output of the previous block\n",
    "        # generated: output of the current block\n",
    "\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "    \n",
    "    def forward(self, x, alpha, steps):     # if steps =0 (4x4), if steps = 1 (8x8) ...\n",
    "        # x: latent vector\n",
    "        # alpha: fade in factor\n",
    "        # steps: number of steps to take\n",
    "\n",
    "        out = self.initial(x)\n",
    "        upscaled = self.initial_rgb(x)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)       # rgb is always one step behind\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList()\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # create the convolutional blocks. Start from the highest resolution and work our way down\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c, use_pixelnorm=False))\n",
    "            self.rgb_layers.append(WSConv2d(img_channels, conv_in_c, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "        # this is for the 4x4 image resolution\n",
    "        self.initial_rgb = WSConv2d(img_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # block for the 4x4 image resolution\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, 1, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "    \n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std_dev(self, x):\n",
    "        batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])    # calculate the standard deviation of the batch for each channel\n",
    "        return torch.cat([x, batch_statistics], dim=1)     # concatenate the batch statistics to the input tensor. 512 -> 513 channels\n",
    "    \n",
    "    def forward(self, x, alpha, steps):     # if steps =0 (4x4), if steps = 1 (8x8) ...\n",
    "        current_step = len(self.prog_blocks) - steps\n",
    "        out = self.leaky(self.rgb_layers[current_step](x))\n",
    "\n",
    "        # if steps == 0, we are at the 4x4 resolution\n",
    "        if steps == 0:\n",
    "           out = self.minibatch_std_dev(out)\n",
    "           return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        downscaled = self.leaky(self.rgb_layers[current_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[current_step](out))\n",
    "\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        # add one to the current step because we have already done the first right above\n",
    "        #   this for loop is for the remaining steps to just downsample the image\n",
    "        for step in range(current_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std_dev(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\callahan\\AppData\\Local\\Temp\\ipykernel_4552\\1960076641.py:33: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1823.)\n",
      "  batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])    # calculate the standard deviation of the batch for each channel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed for image size 4\n",
      "Passed for image size 8\n",
      "Passed for image size 16\n",
      "Passed for image size 32\n",
      "Passed for image size 64\n",
      "Passed for image size 128\n",
      "Passed for image size 256\n",
      "Passed for image size 512\n",
      "Passed for image size 1024\n"
     ]
    }
   ],
   "source": [
    "Z_DIM = 256\n",
    "IN_CHANNELS = 256\n",
    "\n",
    "gen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "disc = Discriminator(IN_CHANNELS, img_channels=3)\n",
    "\n",
    "for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "    num_steps = int(log2(img_size / 4))\n",
    "    x = torch.randn((1, Z_DIM, 1, 1))\n",
    "    z = gen(x, alpha=0.5, steps=num_steps)\n",
    "    assert z.shape == (1, 3, img_size, img_size)\n",
    "    out = disc(z, alpha=0.5, steps=num_steps)\n",
    "    assert out.shape == (1, 1)\n",
    "    print(f\"Passed for image size {img_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Train the Model</h2>\n",
    "    <p>\n",
    "        ...\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\callahan\\AppData\\Local\\Temp\\ipykernel_4552\\2441517094.py:81: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_disc = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\callahan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "C:\\Users\\callahan\\AppData\\Local\\Temp\\ipykernel_4552\\2441517094.py:82: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_gen = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 111\u001b[0m\n\u001b[0;32m    107\u001b[0m         step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 111\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 89\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m tensorboard_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     87\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(log2(START_TRAIN_AT_IMG_SIZE \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m)) \u001b[38;5;66;03m# is going to be 0 when we start training at 4x4\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_epochs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPROGRESSIVE_EPOCHS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     90\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     91\u001b[0m     loader, dataset \u001b[38;5;241m=\u001b[39m get_loader(\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import tqdm as tqdm\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.backends.cudnn.benchmark = True # Improves performance if input sizes don't change\n",
    "\n",
    "def get_loader(image_size):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.Normalize(\n",
    "                [0.5 for _ in range(CHANNELS_IMG)],\n",
    "                [0.5 for _ in range(CHANNELS_IMG)],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n",
    "    dataset = datasets.ImageFolder(root=dataset, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader, dataset\n",
    "    pass\n",
    "\n",
    "def train_fn(disc, gen, loader, dataset, step, alpha, gen_opt, disc_opt, tensorboard_step, scaler_disc, scaler_gen):\n",
    "\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        # Train Discriminator: max E[critic(real)] - E[critic(fake)]\n",
    "        # we use maximize as we dont want these two to be close to each other\n",
    "        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(noise, alpha, step)\n",
    "            disc_real = disc(real, alpha, step)\n",
    "            disc_fake = disc(fake.detach(), alpha, step)\n",
    "            gp = gradient_penalty(disc, real, fake, alpha, step)\n",
    "\n",
    "            loss_disc = (\n",
    "            -(torch.mean(disc_real) - torch.mean(disc_fake)) \n",
    "            + LAMDA_GP * gp\n",
    "            + (0.001 * torch.mean(disc_real ** 2)) # ensure the critic doesn't get too far away from zero. If it is too far, the generator will struggle to learn\n",
    "            )\n",
    "\n",
    "            disc_opt.zero_grad()\n",
    "            scaler_disc.scale(loss_disc).backward()\n",
    "            scaler_disc.step(disc_opt)\n",
    "            scaler_disc.update()\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake)]\n",
    "        # we use maximize as we want the generator to fool the critic\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = disc(fake, alpha, step)\n",
    "            loss_gen = -torch.mean(output)\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(gen_opt)\n",
    "        scaler_gen.update()\n",
    "\n",
    "        alpha += cur_batch_size / (len(dataset) * 0.5 * PROGRESSIVE_EPOCHS[step] )\n",
    "        alpha = min(alpha, 1)       # ensure alpha doesn't go above 1\n",
    "\n",
    "\n",
    "def main():\n",
    "    gen = Generator(Z_DIM, IN_CHANNELS, Image_Size)\n",
    "    disc = Discriminator(IN_CHANNELS, Image_Size)\n",
    "\n",
    "    # initialize the optimizers\n",
    "    gen_opt = torch.optim.Adam(gen.parameters(), lr=Learning_Rate, betas=(0.0, 0.99))   # betas are the exponential decay rates for the first and second moment estimates\n",
    "    disc_opt = torch.optim.Adam(disc.parameters(), lr=Learning_Rate, betas=(0.0, 0.99))\n",
    "    scaler_disc = torch.cuda.amp.GradScaler()\n",
    "    scaler_gen = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "    tensorboard_step = 0\n",
    "    step = int(log2(START_TRAIN_AT_IMG_SIZE / 4)) # is going to be 0 when we start training at 4x4\n",
    "\n",
    "    for num_epochs in range(PROGRESSIVE_EPOCHS[step:]):\n",
    "        alpha = 0\n",
    "        loader, dataset = get_loader(4 * 2 ** step)\n",
    "\n",
    "        print(f\"Image Size: {4 * 2 ** step}\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "            tensorboard_step, alpha = train_fn(\n",
    "                disc, gen, loader, dataset, step, alpha, gen_opt, disc_opt, tensorboard_step, scaler_disc, scaler_gen\n",
    "            )\n",
    "\n",
    "            # If save model is true, save the model every epoch\n",
    "            if Save_Model:\n",
    "                save_checkpoint(gen, gen_opt, filename=CHECKPOINT_GEN)\n",
    "                save_checkpoint(disc, disc_opt, filename=CHECKPOINT_DISC)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Test the Model</h2>\n",
    "    <p>\n",
    "        ...\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Analyze the Results</h2>\n",
    "    <p>\n",
    "        ...\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>Conclusion</h2>\n",
    "    <p>\n",
    "        ...\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Default styles for light mode */\n",
    "    .content {\n",
    "        font-family: 'Georgia', serif;\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        line-height: 1.6;\n",
    "        font-size: 20px;\n",
    "        color: #333333; /* Dark text for light mode */\n",
    "        text-align: justify;\n",
    "    }\n",
    "\n",
    "    .content h2 {\n",
    "        font-weight: 700;\n",
    "        font-size: 36px;\n",
    "        color: #333333; /* Dark color for light mode */\n",
    "        margin-bottom: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    .content p {\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    .content blockquote {\n",
    "        border-left: 4px solid #BBBBBB;\n",
    "        padding-left: 20px;\n",
    "        margin: 30px 0;\n",
    "        font-style: italic;\n",
    "        color: #666666; /* Medium grey for light mode */\n",
    "    }\n",
    "\n",
    "    /* Styles for dark mode */\n",
    "    @media (prefers-color-scheme: dark) {\n",
    "        .content {\n",
    "            color: #DDDDDD; /* Light text for dark mode */\n",
    "        }\n",
    "\n",
    "        .content h2 {\n",
    "            color: #F5F5F5; /* Light color for dark mode */\n",
    "        }\n",
    "\n",
    "        .content blockquote {\n",
    "            border-left: 4px solid #555555;\n",
    "            color: #BBBBBB; /* Light grey for dark mode */\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"content\">\n",
    "    <h2>References</h2>\n",
    "    <p>\n",
    "        ...\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
